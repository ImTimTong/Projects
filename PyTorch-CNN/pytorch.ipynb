{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ConvNet Implementation \"\"\""
      ],
      "metadata": {
        "id": "uQQI1Qv4bjWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 200\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Only normalization for testing\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Custom CNN model\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(512 * 1 * 1, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
        "        self.batch_norm4 = nn.BatchNorm2d(512)\n",
        "        self.batch_norm5 = nn.BatchNorm2d(512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.batch_norm4(self.conv4(x))))\n",
        "        x = self.pool(F.relu(self.batch_norm5(self.conv5(x))))\n",
        "\n",
        "        x = x.view(-1, 512 * 1 * 1)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = CustomCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "def train(model, train_loader, criterion, optimizer, scheduler, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        scheduler.step(running_loss)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "# Testing loop\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        class_correct = [0 for _ in range(10)]\n",
        "        class_samples = [0 for _ in range(10)]\n",
        "\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            n_samples += labels.size(0)\n",
        "            n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "                if label == pred:\n",
        "                    class_correct[label] += 1\n",
        "                class_samples[label] += 1\n",
        "\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "        print(f'Accuracy of the network: {acc:.2f}%')\n",
        "\n",
        "        for i in range(10):\n",
        "            if class_samples[i] != 0:\n",
        "                acc = 100.0 * class_correct[i] / class_samples[i]\n",
        "                print(f'Accuracy of {classes[i]}: {acc:.2f}%')\n",
        "            else:\n",
        "                print(f'Accuracy of {classes[i]}: N/A (no samples)')\n",
        "\n",
        "test(model, test_loader)"
      ],
      "metadata": {
        "id": "Ny6CJvKcm1S7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db40dbda-39f4-484f-8753-0d8121d1c8d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 51634560.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Step [100/391], Loss: 1.7967\n",
            "Epoch [1/200], Step [200/391], Loss: 1.7288\n",
            "Epoch [1/200], Step [300/391], Loss: 1.3527\n",
            "Epoch [2/200], Step [100/391], Loss: 1.3373\n",
            "Epoch [2/200], Step [200/391], Loss: 1.1826\n",
            "Epoch [2/200], Step [300/391], Loss: 1.2976\n",
            "Epoch [3/200], Step [100/391], Loss: 1.0463\n",
            "Epoch [3/200], Step [200/391], Loss: 1.1074\n",
            "Epoch [3/200], Step [300/391], Loss: 1.3151\n",
            "Epoch [4/200], Step [100/391], Loss: 1.1604\n",
            "Epoch [4/200], Step [200/391], Loss: 1.0295\n",
            "Epoch [4/200], Step [300/391], Loss: 1.0698\n",
            "Epoch [5/200], Step [100/391], Loss: 0.9765\n",
            "Epoch [5/200], Step [200/391], Loss: 0.9706\n",
            "Epoch [5/200], Step [300/391], Loss: 1.0711\n",
            "Epoch [6/200], Step [100/391], Loss: 0.9845\n",
            "Epoch [6/200], Step [200/391], Loss: 0.9290\n",
            "Epoch [6/200], Step [300/391], Loss: 0.7957\n",
            "Epoch [7/200], Step [100/391], Loss: 0.9795\n",
            "Epoch [7/200], Step [200/391], Loss: 0.7156\n",
            "Epoch [7/200], Step [300/391], Loss: 0.8171\n",
            "Epoch [8/200], Step [100/391], Loss: 0.8092\n",
            "Epoch [8/200], Step [200/391], Loss: 0.8382\n",
            "Epoch [8/200], Step [300/391], Loss: 0.6867\n",
            "Epoch [9/200], Step [100/391], Loss: 0.9026\n",
            "Epoch [9/200], Step [200/391], Loss: 0.8388\n",
            "Epoch [9/200], Step [300/391], Loss: 0.9732\n",
            "Epoch [10/200], Step [100/391], Loss: 0.7720\n",
            "Epoch [10/200], Step [200/391], Loss: 0.5810\n",
            "Epoch [10/200], Step [300/391], Loss: 0.7389\n",
            "Epoch [11/200], Step [100/391], Loss: 0.8472\n",
            "Epoch [11/200], Step [200/391], Loss: 0.6419\n",
            "Epoch [11/200], Step [300/391], Loss: 0.6308\n",
            "Epoch [12/200], Step [100/391], Loss: 0.5977\n",
            "Epoch [12/200], Step [200/391], Loss: 0.4846\n",
            "Epoch [12/200], Step [300/391], Loss: 0.6126\n",
            "Epoch [13/200], Step [100/391], Loss: 0.6075\n",
            "Epoch [13/200], Step [200/391], Loss: 0.6198\n",
            "Epoch [13/200], Step [300/391], Loss: 0.7418\n",
            "Epoch [14/200], Step [100/391], Loss: 0.6245\n",
            "Epoch [14/200], Step [200/391], Loss: 0.7154\n",
            "Epoch [14/200], Step [300/391], Loss: 0.5819\n",
            "Epoch [15/200], Step [100/391], Loss: 0.7136\n",
            "Epoch [15/200], Step [200/391], Loss: 0.5206\n",
            "Epoch [15/200], Step [300/391], Loss: 0.7427\n",
            "Epoch [16/200], Step [100/391], Loss: 0.5809\n",
            "Epoch [16/200], Step [200/391], Loss: 0.6219\n",
            "Epoch [16/200], Step [300/391], Loss: 0.4923\n",
            "Epoch [17/200], Step [100/391], Loss: 0.6740\n",
            "Epoch [17/200], Step [200/391], Loss: 0.5316\n",
            "Epoch [17/200], Step [300/391], Loss: 0.6042\n",
            "Epoch [18/200], Step [100/391], Loss: 0.5697\n",
            "Epoch [18/200], Step [200/391], Loss: 0.5371\n",
            "Epoch [18/200], Step [300/391], Loss: 0.4496\n",
            "Epoch [19/200], Step [100/391], Loss: 0.6477\n",
            "Epoch [19/200], Step [200/391], Loss: 0.5730\n",
            "Epoch [19/200], Step [300/391], Loss: 0.6746\n",
            "Epoch [20/200], Step [100/391], Loss: 0.5790\n",
            "Epoch [20/200], Step [200/391], Loss: 0.4877\n",
            "Epoch [20/200], Step [300/391], Loss: 0.4205\n",
            "Epoch [21/200], Step [100/391], Loss: 0.5623\n",
            "Epoch [21/200], Step [200/391], Loss: 0.4893\n",
            "Epoch [21/200], Step [300/391], Loss: 0.4953\n",
            "Epoch [22/200], Step [100/391], Loss: 0.4702\n",
            "Epoch [22/200], Step [200/391], Loss: 0.3564\n",
            "Epoch [22/200], Step [300/391], Loss: 0.4138\n",
            "Epoch [23/200], Step [100/391], Loss: 0.3638\n",
            "Epoch [23/200], Step [200/391], Loss: 0.6628\n",
            "Epoch [23/200], Step [300/391], Loss: 0.4693\n",
            "Epoch [24/200], Step [100/391], Loss: 0.4077\n",
            "Epoch [24/200], Step [200/391], Loss: 0.4853\n",
            "Epoch [24/200], Step [300/391], Loss: 0.5828\n",
            "Epoch [25/200], Step [100/391], Loss: 0.4670\n",
            "Epoch [25/200], Step [200/391], Loss: 0.4072\n",
            "Epoch [25/200], Step [300/391], Loss: 0.4273\n",
            "Epoch [26/200], Step [100/391], Loss: 0.6001\n",
            "Epoch [26/200], Step [200/391], Loss: 0.4197\n",
            "Epoch [26/200], Step [300/391], Loss: 0.4047\n",
            "Epoch [27/200], Step [100/391], Loss: 0.4605\n",
            "Epoch [27/200], Step [200/391], Loss: 0.3826\n",
            "Epoch [27/200], Step [300/391], Loss: 0.7264\n",
            "Epoch [28/200], Step [100/391], Loss: 0.3042\n",
            "Epoch [28/200], Step [200/391], Loss: 0.3937\n",
            "Epoch [28/200], Step [300/391], Loss: 0.5008\n",
            "Epoch [29/200], Step [100/391], Loss: 0.5245\n",
            "Epoch [29/200], Step [200/391], Loss: 0.4335\n",
            "Epoch [29/200], Step [300/391], Loss: 0.3851\n",
            "Epoch [30/200], Step [100/391], Loss: 0.4225\n",
            "Epoch [30/200], Step [200/391], Loss: 0.3857\n",
            "Epoch [30/200], Step [300/391], Loss: 0.3542\n",
            "Epoch [31/200], Step [100/391], Loss: 0.3366\n",
            "Epoch [31/200], Step [200/391], Loss: 0.4000\n",
            "Epoch [31/200], Step [300/391], Loss: 0.3518\n",
            "Epoch [32/200], Step [100/391], Loss: 0.4040\n",
            "Epoch [32/200], Step [200/391], Loss: 0.5026\n",
            "Epoch [32/200], Step [300/391], Loss: 0.3322\n",
            "Epoch [33/200], Step [100/391], Loss: 0.5062\n",
            "Epoch [33/200], Step [200/391], Loss: 0.3841\n",
            "Epoch [33/200], Step [300/391], Loss: 0.4755\n",
            "Epoch [34/200], Step [100/391], Loss: 0.3511\n",
            "Epoch [34/200], Step [200/391], Loss: 0.4036\n",
            "Epoch [34/200], Step [300/391], Loss: 0.3862\n",
            "Epoch [35/200], Step [100/391], Loss: 0.3392\n",
            "Epoch [35/200], Step [200/391], Loss: 0.5197\n",
            "Epoch [35/200], Step [300/391], Loss: 0.3809\n",
            "Epoch [36/200], Step [100/391], Loss: 0.3530\n",
            "Epoch [36/200], Step [200/391], Loss: 0.3947\n",
            "Epoch [36/200], Step [300/391], Loss: 0.3887\n",
            "Epoch [37/200], Step [100/391], Loss: 0.5100\n",
            "Epoch [37/200], Step [200/391], Loss: 0.3426\n",
            "Epoch [37/200], Step [300/391], Loss: 0.3710\n",
            "Epoch [38/200], Step [100/391], Loss: 0.3860\n",
            "Epoch [38/200], Step [200/391], Loss: 0.3897\n",
            "Epoch [38/200], Step [300/391], Loss: 0.3105\n",
            "Epoch [39/200], Step [100/391], Loss: 0.3643\n",
            "Epoch [39/200], Step [200/391], Loss: 0.2440\n",
            "Epoch [39/200], Step [300/391], Loss: 0.4071\n",
            "Epoch [40/200], Step [100/391], Loss: 0.3395\n",
            "Epoch [40/200], Step [200/391], Loss: 0.3818\n",
            "Epoch [40/200], Step [300/391], Loss: 0.4374\n",
            "Epoch [41/200], Step [100/391], Loss: 0.3578\n",
            "Epoch [41/200], Step [200/391], Loss: 0.2350\n",
            "Epoch [41/200], Step [300/391], Loss: 0.3614\n",
            "Epoch [42/200], Step [100/391], Loss: 0.3359\n",
            "Epoch [42/200], Step [200/391], Loss: 0.3536\n",
            "Epoch [42/200], Step [300/391], Loss: 0.3489\n",
            "Epoch [43/200], Step [100/391], Loss: 0.4568\n",
            "Epoch [43/200], Step [200/391], Loss: 0.3786\n",
            "Epoch [43/200], Step [300/391], Loss: 0.2146\n",
            "Epoch [44/200], Step [100/391], Loss: 0.3971\n",
            "Epoch [44/200], Step [200/391], Loss: 0.3953\n",
            "Epoch [44/200], Step [300/391], Loss: 0.2367\n",
            "Epoch [45/200], Step [100/391], Loss: 0.2785\n",
            "Epoch [45/200], Step [200/391], Loss: 0.3233\n",
            "Epoch [45/200], Step [300/391], Loss: 0.4774\n",
            "Epoch [46/200], Step [100/391], Loss: 0.4606\n",
            "Epoch [46/200], Step [200/391], Loss: 0.2350\n",
            "Epoch [46/200], Step [300/391], Loss: 0.3431\n",
            "Epoch [47/200], Step [100/391], Loss: 0.3393\n",
            "Epoch [47/200], Step [200/391], Loss: 0.3182\n",
            "Epoch [47/200], Step [300/391], Loss: 0.1800\n",
            "Epoch [48/200], Step [100/391], Loss: 0.3516\n",
            "Epoch [48/200], Step [200/391], Loss: 0.2912\n",
            "Epoch [48/200], Step [300/391], Loss: 0.3677\n",
            "Epoch [49/200], Step [100/391], Loss: 0.3452\n",
            "Epoch [49/200], Step [200/391], Loss: 0.3014\n",
            "Epoch [49/200], Step [300/391], Loss: 0.3465\n",
            "Epoch [50/200], Step [100/391], Loss: 0.3224\n",
            "Epoch [50/200], Step [200/391], Loss: 0.3260\n",
            "Epoch [50/200], Step [300/391], Loss: 0.3780\n",
            "Epoch [51/200], Step [100/391], Loss: 0.2828\n",
            "Epoch [51/200], Step [200/391], Loss: 0.2646\n",
            "Epoch [51/200], Step [300/391], Loss: 0.3170\n",
            "Epoch [52/200], Step [100/391], Loss: 0.2737\n",
            "Epoch [52/200], Step [200/391], Loss: 0.2896\n",
            "Epoch [52/200], Step [300/391], Loss: 0.2924\n",
            "Epoch [53/200], Step [100/391], Loss: 0.1901\n",
            "Epoch [53/200], Step [200/391], Loss: 0.2948\n",
            "Epoch [53/200], Step [300/391], Loss: 0.3291\n",
            "Epoch [54/200], Step [100/391], Loss: 0.3461\n",
            "Epoch [54/200], Step [200/391], Loss: 0.2770\n",
            "Epoch [54/200], Step [300/391], Loss: 0.3098\n",
            "Epoch [55/200], Step [100/391], Loss: 0.3750\n",
            "Epoch [55/200], Step [200/391], Loss: 0.2486\n",
            "Epoch [55/200], Step [300/391], Loss: 0.1929\n",
            "Epoch [56/200], Step [100/391], Loss: 0.3353\n",
            "Epoch [56/200], Step [200/391], Loss: 0.2788\n",
            "Epoch [56/200], Step [300/391], Loss: 0.2430\n",
            "Epoch [57/200], Step [100/391], Loss: 0.2557\n",
            "Epoch [57/200], Step [200/391], Loss: 0.3039\n",
            "Epoch [57/200], Step [300/391], Loss: 0.1441\n",
            "Epoch [58/200], Step [100/391], Loss: 0.1083\n",
            "Epoch [58/200], Step [200/391], Loss: 0.1765\n",
            "Epoch [58/200], Step [300/391], Loss: 0.3805\n",
            "Epoch [59/200], Step [100/391], Loss: 0.2440\n",
            "Epoch [59/200], Step [200/391], Loss: 0.2064\n",
            "Epoch [59/200], Step [300/391], Loss: 0.1108\n",
            "Epoch [60/200], Step [100/391], Loss: 0.2463\n",
            "Epoch [60/200], Step [200/391], Loss: 0.2533\n",
            "Epoch [60/200], Step [300/391], Loss: 0.3330\n",
            "Epoch [61/200], Step [100/391], Loss: 0.1844\n",
            "Epoch [61/200], Step [200/391], Loss: 0.2941\n",
            "Epoch [61/200], Step [300/391], Loss: 0.3604\n",
            "Epoch [62/200], Step [100/391], Loss: 0.1313\n",
            "Epoch [62/200], Step [200/391], Loss: 0.1749\n",
            "Epoch [62/200], Step [300/391], Loss: 0.1581\n",
            "Epoch [63/200], Step [100/391], Loss: 0.3086\n",
            "Epoch [63/200], Step [200/391], Loss: 0.2530\n",
            "Epoch [63/200], Step [300/391], Loss: 0.2217\n",
            "Epoch [64/200], Step [100/391], Loss: 0.2590\n",
            "Epoch [64/200], Step [200/391], Loss: 0.2601\n",
            "Epoch [64/200], Step [300/391], Loss: 0.3117\n",
            "Epoch [65/200], Step [100/391], Loss: 0.1076\n",
            "Epoch [65/200], Step [200/391], Loss: 0.2329\n",
            "Epoch [65/200], Step [300/391], Loss: 0.3847\n",
            "Epoch [66/200], Step [100/391], Loss: 0.2572\n",
            "Epoch [66/200], Step [200/391], Loss: 0.2664\n",
            "Epoch [66/200], Step [300/391], Loss: 0.2239\n",
            "Epoch [67/200], Step [100/391], Loss: 0.2730\n",
            "Epoch [67/200], Step [200/391], Loss: 0.1849\n",
            "Epoch [67/200], Step [300/391], Loss: 0.2533\n",
            "Epoch [68/200], Step [100/391], Loss: 0.1752\n",
            "Epoch [68/200], Step [200/391], Loss: 0.1800\n",
            "Epoch [68/200], Step [300/391], Loss: 0.2564\n",
            "Epoch [69/200], Step [100/391], Loss: 0.2595\n",
            "Epoch [69/200], Step [200/391], Loss: 0.0860\n",
            "Epoch [69/200], Step [300/391], Loss: 0.2139\n",
            "Epoch [70/200], Step [100/391], Loss: 0.1440\n",
            "Epoch [70/200], Step [200/391], Loss: 0.1551\n",
            "Epoch [70/200], Step [300/391], Loss: 0.2012\n",
            "Epoch [71/200], Step [100/391], Loss: 0.2363\n",
            "Epoch [71/200], Step [200/391], Loss: 0.1472\n",
            "Epoch [71/200], Step [300/391], Loss: 0.1891\n",
            "Epoch [72/200], Step [100/391], Loss: 0.2646\n",
            "Epoch [72/200], Step [200/391], Loss: 0.1965\n",
            "Epoch [72/200], Step [300/391], Loss: 0.1234\n",
            "Epoch [73/200], Step [100/391], Loss: 0.3019\n",
            "Epoch [73/200], Step [200/391], Loss: 0.1789\n",
            "Epoch [73/200], Step [300/391], Loss: 0.1667\n",
            "Epoch [74/200], Step [100/391], Loss: 0.2100\n",
            "Epoch [74/200], Step [200/391], Loss: 0.2466\n",
            "Epoch [74/200], Step [300/391], Loss: 0.1950\n",
            "Epoch [75/200], Step [100/391], Loss: 0.2066\n",
            "Epoch [75/200], Step [200/391], Loss: 0.1858\n",
            "Epoch [75/200], Step [300/391], Loss: 0.2947\n",
            "Epoch [76/200], Step [100/391], Loss: 0.1624\n",
            "Epoch [76/200], Step [200/391], Loss: 0.1289\n",
            "Epoch [76/200], Step [300/391], Loss: 0.2532\n",
            "Epoch [77/200], Step [100/391], Loss: 0.1598\n",
            "Epoch [77/200], Step [200/391], Loss: 0.2286\n",
            "Epoch [77/200], Step [300/391], Loss: 0.2390\n",
            "Epoch [78/200], Step [100/391], Loss: 0.1633\n",
            "Epoch [78/200], Step [200/391], Loss: 0.2056\n",
            "Epoch [78/200], Step [300/391], Loss: 0.2376\n",
            "Epoch [79/200], Step [100/391], Loss: 0.2352\n",
            "Epoch [79/200], Step [200/391], Loss: 0.2997\n",
            "Epoch [79/200], Step [300/391], Loss: 0.1656\n",
            "Epoch [80/200], Step [100/391], Loss: 0.1502\n",
            "Epoch [80/200], Step [200/391], Loss: 0.1009\n",
            "Epoch [80/200], Step [300/391], Loss: 0.2334\n",
            "Epoch [81/200], Step [100/391], Loss: 0.2352\n",
            "Epoch [81/200], Step [200/391], Loss: 0.2036\n",
            "Epoch [81/200], Step [300/391], Loss: 0.2083\n",
            "Epoch [82/200], Step [100/391], Loss: 0.1222\n",
            "Epoch [82/200], Step [200/391], Loss: 0.1918\n",
            "Epoch [82/200], Step [300/391], Loss: 0.1413\n",
            "Epoch [83/200], Step [100/391], Loss: 0.1824\n",
            "Epoch [83/200], Step [200/391], Loss: 0.1015\n",
            "Epoch [83/200], Step [300/391], Loss: 0.1553\n",
            "Epoch [84/200], Step [100/391], Loss: 0.1942\n",
            "Epoch [84/200], Step [200/391], Loss: 0.2557\n",
            "Epoch [84/200], Step [300/391], Loss: 0.2558\n",
            "Epoch [85/200], Step [100/391], Loss: 0.1933\n",
            "Epoch [85/200], Step [200/391], Loss: 0.2157\n",
            "Epoch [85/200], Step [300/391], Loss: 0.1940\n",
            "Epoch [86/200], Step [100/391], Loss: 0.0993\n",
            "Epoch [86/200], Step [200/391], Loss: 0.2411\n",
            "Epoch [86/200], Step [300/391], Loss: 0.1653\n",
            "Epoch [87/200], Step [100/391], Loss: 0.1716\n",
            "Epoch [87/200], Step [200/391], Loss: 0.1914\n",
            "Epoch [87/200], Step [300/391], Loss: 0.1197\n",
            "Epoch [88/200], Step [100/391], Loss: 0.1598\n",
            "Epoch [88/200], Step [200/391], Loss: 0.2274\n",
            "Epoch [88/200], Step [300/391], Loss: 0.1980\n",
            "Epoch [89/200], Step [100/391], Loss: 0.1509\n",
            "Epoch [89/200], Step [200/391], Loss: 0.1351\n",
            "Epoch [89/200], Step [300/391], Loss: 0.1981\n",
            "Epoch [90/200], Step [100/391], Loss: 0.1726\n",
            "Epoch [90/200], Step [200/391], Loss: 0.1828\n",
            "Epoch [90/200], Step [300/391], Loss: 0.2259\n",
            "Epoch [91/200], Step [100/391], Loss: 0.0497\n",
            "Epoch [91/200], Step [200/391], Loss: 0.1778\n",
            "Epoch [91/200], Step [300/391], Loss: 0.2562\n",
            "Epoch [92/200], Step [100/391], Loss: 0.1576\n",
            "Epoch [92/200], Step [200/391], Loss: 0.1527\n",
            "Epoch [92/200], Step [300/391], Loss: 0.1210\n",
            "Epoch [93/200], Step [100/391], Loss: 0.1558\n",
            "Epoch [93/200], Step [200/391], Loss: 0.2175\n",
            "Epoch [93/200], Step [300/391], Loss: 0.1784\n",
            "Epoch [94/200], Step [100/391], Loss: 0.1141\n",
            "Epoch [94/200], Step [200/391], Loss: 0.1630\n",
            "Epoch [94/200], Step [300/391], Loss: 0.1124\n",
            "Epoch [95/200], Step [100/391], Loss: 0.1711\n",
            "Epoch [95/200], Step [200/391], Loss: 0.2076\n",
            "Epoch [95/200], Step [300/391], Loss: 0.1384\n",
            "Epoch [96/200], Step [100/391], Loss: 0.0777\n",
            "Epoch [96/200], Step [200/391], Loss: 0.1765\n",
            "Epoch [96/200], Step [300/391], Loss: 0.1187\n",
            "Epoch [97/200], Step [100/391], Loss: 0.3552\n",
            "Epoch [97/200], Step [200/391], Loss: 0.2036\n",
            "Epoch [97/200], Step [300/391], Loss: 0.1168\n",
            "Epoch [98/200], Step [100/391], Loss: 0.1973\n",
            "Epoch [98/200], Step [200/391], Loss: 0.2004\n",
            "Epoch [98/200], Step [300/391], Loss: 0.1474\n",
            "Epoch [99/200], Step [100/391], Loss: 0.1776\n",
            "Epoch [99/200], Step [200/391], Loss: 0.1352\n",
            "Epoch [99/200], Step [300/391], Loss: 0.1421\n",
            "Epoch [100/200], Step [100/391], Loss: 0.2503\n",
            "Epoch [100/200], Step [200/391], Loss: 0.0950\n",
            "Epoch [100/200], Step [300/391], Loss: 0.0984\n",
            "Epoch [101/200], Step [100/391], Loss: 0.0509\n",
            "Epoch [101/200], Step [200/391], Loss: 0.2223\n",
            "Epoch [101/200], Step [300/391], Loss: 0.1011\n",
            "Epoch [102/200], Step [100/391], Loss: 0.1306\n",
            "Epoch [102/200], Step [200/391], Loss: 0.0872\n",
            "Epoch [102/200], Step [300/391], Loss: 0.2146\n",
            "Epoch [103/200], Step [100/391], Loss: 0.0777\n",
            "Epoch [103/200], Step [200/391], Loss: 0.1523\n",
            "Epoch [103/200], Step [300/391], Loss: 0.2139\n",
            "Epoch [104/200], Step [100/391], Loss: 0.0578\n",
            "Epoch [104/200], Step [200/391], Loss: 0.1737\n",
            "Epoch [104/200], Step [300/391], Loss: 0.1688\n",
            "Epoch [105/200], Step [100/391], Loss: 0.1527\n",
            "Epoch [105/200], Step [200/391], Loss: 0.1939\n",
            "Epoch [105/200], Step [300/391], Loss: 0.1670\n",
            "Epoch [106/200], Step [100/391], Loss: 0.0978\n",
            "Epoch [106/200], Step [200/391], Loss: 0.0840\n",
            "Epoch [106/200], Step [300/391], Loss: 0.1438\n",
            "Epoch [107/200], Step [100/391], Loss: 0.0693\n",
            "Epoch [107/200], Step [200/391], Loss: 0.1577\n",
            "Epoch [107/200], Step [300/391], Loss: 0.0960\n",
            "Epoch [108/200], Step [100/391], Loss: 0.1000\n",
            "Epoch [108/200], Step [200/391], Loss: 0.1581\n",
            "Epoch [108/200], Step [300/391], Loss: 0.0622\n",
            "Epoch [109/200], Step [100/391], Loss: 0.1414\n",
            "Epoch [109/200], Step [200/391], Loss: 0.2027\n",
            "Epoch [109/200], Step [300/391], Loss: 0.0873\n",
            "Epoch [110/200], Step [100/391], Loss: 0.1115\n",
            "Epoch [110/200], Step [200/391], Loss: 0.1209\n",
            "Epoch [110/200], Step [300/391], Loss: 0.0803\n",
            "Epoch [111/200], Step [100/391], Loss: 0.1410\n",
            "Epoch [111/200], Step [200/391], Loss: 0.1238\n",
            "Epoch [111/200], Step [300/391], Loss: 0.0650\n",
            "Epoch [112/200], Step [100/391], Loss: 0.1290\n",
            "Epoch [112/200], Step [200/391], Loss: 0.1365\n",
            "Epoch [112/200], Step [300/391], Loss: 0.1010\n",
            "Epoch [113/200], Step [100/391], Loss: 0.1111\n",
            "Epoch [113/200], Step [200/391], Loss: 0.1634\n",
            "Epoch [113/200], Step [300/391], Loss: 0.1082\n",
            "Epoch [114/200], Step [100/391], Loss: 0.1129\n",
            "Epoch [114/200], Step [200/391], Loss: 0.2042\n",
            "Epoch [114/200], Step [300/391], Loss: 0.1875\n",
            "Epoch [115/200], Step [100/391], Loss: 0.1576\n",
            "Epoch [115/200], Step [200/391], Loss: 0.0934\n",
            "Epoch [115/200], Step [300/391], Loss: 0.1171\n",
            "Epoch [116/200], Step [100/391], Loss: 0.1124\n",
            "Epoch [116/200], Step [200/391], Loss: 0.1118\n",
            "Epoch [116/200], Step [300/391], Loss: 0.0826\n",
            "Epoch [117/200], Step [100/391], Loss: 0.1528\n",
            "Epoch [117/200], Step [200/391], Loss: 0.1196\n",
            "Epoch [117/200], Step [300/391], Loss: 0.1677\n",
            "Epoch [118/200], Step [100/391], Loss: 0.0780\n",
            "Epoch [118/200], Step [200/391], Loss: 0.1123\n",
            "Epoch [118/200], Step [300/391], Loss: 0.2117\n",
            "Epoch [119/200], Step [100/391], Loss: 0.2649\n",
            "Epoch [119/200], Step [200/391], Loss: 0.0688\n",
            "Epoch [119/200], Step [300/391], Loss: 0.1647\n",
            "Epoch [120/200], Step [100/391], Loss: 0.1429\n",
            "Epoch [120/200], Step [200/391], Loss: 0.1663\n",
            "Epoch [120/200], Step [300/391], Loss: 0.0993\n",
            "Epoch [121/200], Step [100/391], Loss: 0.0994\n",
            "Epoch [121/200], Step [200/391], Loss: 0.1255\n",
            "Epoch [121/200], Step [300/391], Loss: 0.1014\n",
            "Epoch [122/200], Step [100/391], Loss: 0.1525\n",
            "Epoch [122/200], Step [200/391], Loss: 0.0847\n",
            "Epoch [122/200], Step [300/391], Loss: 0.1106\n",
            "Epoch [123/200], Step [100/391], Loss: 0.0643\n",
            "Epoch [123/200], Step [200/391], Loss: 0.0649\n",
            "Epoch [123/200], Step [300/391], Loss: 0.1358\n",
            "Epoch [124/200], Step [100/391], Loss: 0.0793\n",
            "Epoch [124/200], Step [200/391], Loss: 0.0898\n",
            "Epoch [124/200], Step [300/391], Loss: 0.1092\n",
            "Epoch [125/200], Step [100/391], Loss: 0.0671\n",
            "Epoch [125/200], Step [200/391], Loss: 0.0370\n",
            "Epoch [125/200], Step [300/391], Loss: 0.0918\n",
            "Epoch [126/200], Step [100/391], Loss: 0.0710\n",
            "Epoch [126/200], Step [200/391], Loss: 0.1155\n",
            "Epoch [126/200], Step [300/391], Loss: 0.0809\n",
            "Epoch [127/200], Step [100/391], Loss: 0.1951\n",
            "Epoch [127/200], Step [200/391], Loss: 0.1573\n",
            "Epoch [127/200], Step [300/391], Loss: 0.1440\n",
            "Epoch [128/200], Step [100/391], Loss: 0.0802\n",
            "Epoch [128/200], Step [200/391], Loss: 0.1640\n",
            "Epoch [128/200], Step [300/391], Loss: 0.1053\n",
            "Epoch [129/200], Step [100/391], Loss: 0.1142\n",
            "Epoch [129/200], Step [200/391], Loss: 0.0847\n",
            "Epoch [129/200], Step [300/391], Loss: 0.0849\n",
            "Epoch [130/200], Step [100/391], Loss: 0.0667\n",
            "Epoch [130/200], Step [200/391], Loss: 0.0188\n",
            "Epoch [130/200], Step [300/391], Loss: 0.0771\n",
            "Epoch [131/200], Step [100/391], Loss: 0.0783\n",
            "Epoch [131/200], Step [200/391], Loss: 0.1129\n",
            "Epoch [131/200], Step [300/391], Loss: 0.1090\n",
            "Epoch [132/200], Step [100/391], Loss: 0.0802\n",
            "Epoch [132/200], Step [200/391], Loss: 0.2009\n",
            "Epoch [132/200], Step [300/391], Loss: 0.1772\n",
            "Epoch [133/200], Step [100/391], Loss: 0.1545\n",
            "Epoch [133/200], Step [200/391], Loss: 0.1710\n",
            "Epoch [133/200], Step [300/391], Loss: 0.0755\n",
            "Epoch [134/200], Step [100/391], Loss: 0.1508\n",
            "Epoch [134/200], Step [200/391], Loss: 0.1540\n",
            "Epoch [134/200], Step [300/391], Loss: 0.1471\n",
            "Epoch [135/200], Step [100/391], Loss: 0.0910\n",
            "Epoch [135/200], Step [200/391], Loss: 0.0672\n",
            "Epoch [135/200], Step [300/391], Loss: 0.0650\n",
            "Epoch [136/200], Step [100/391], Loss: 0.1282\n",
            "Epoch [136/200], Step [200/391], Loss: 0.0940\n",
            "Epoch [136/200], Step [300/391], Loss: 0.0698\n",
            "Epoch [137/200], Step [100/391], Loss: 0.1430\n",
            "Epoch [137/200], Step [200/391], Loss: 0.1251\n",
            "Epoch [137/200], Step [300/391], Loss: 0.1860\n",
            "Epoch [138/200], Step [100/391], Loss: 0.1550\n",
            "Epoch [138/200], Step [200/391], Loss: 0.1221\n",
            "Epoch [138/200], Step [300/391], Loss: 0.0669\n",
            "Epoch [139/200], Step [100/391], Loss: 0.0667\n",
            "Epoch [139/200], Step [200/391], Loss: 0.1451\n",
            "Epoch [139/200], Step [300/391], Loss: 0.1011\n",
            "Epoch [140/200], Step [100/391], Loss: 0.0895\n",
            "Epoch [140/200], Step [200/391], Loss: 0.1072\n",
            "Epoch [140/200], Step [300/391], Loss: 0.0524\n",
            "Epoch [141/200], Step [100/391], Loss: 0.0957\n",
            "Epoch [141/200], Step [200/391], Loss: 0.1445\n",
            "Epoch [141/200], Step [300/391], Loss: 0.1079\n",
            "Epoch [142/200], Step [100/391], Loss: 0.1007\n",
            "Epoch [142/200], Step [200/391], Loss: 0.1602\n",
            "Epoch [142/200], Step [300/391], Loss: 0.0527\n",
            "Epoch [143/200], Step [100/391], Loss: 0.1279\n",
            "Epoch [143/200], Step [200/391], Loss: 0.0628\n",
            "Epoch [143/200], Step [300/391], Loss: 0.1199\n",
            "Epoch [144/200], Step [100/391], Loss: 0.0514\n",
            "Epoch [144/200], Step [200/391], Loss: 0.1608\n",
            "Epoch [144/200], Step [300/391], Loss: 0.0436\n",
            "Epoch [145/200], Step [100/391], Loss: 0.1024\n",
            "Epoch [145/200], Step [200/391], Loss: 0.1701\n",
            "Epoch [145/200], Step [300/391], Loss: 0.2469\n",
            "Epoch [146/200], Step [100/391], Loss: 0.0873\n",
            "Epoch [146/200], Step [200/391], Loss: 0.1225\n",
            "Epoch [146/200], Step [300/391], Loss: 0.1638\n",
            "Epoch [147/200], Step [100/391], Loss: 0.0684\n",
            "Epoch [147/200], Step [200/391], Loss: 0.0922\n",
            "Epoch [147/200], Step [300/391], Loss: 0.0657\n",
            "Epoch [148/200], Step [100/391], Loss: 0.0798\n",
            "Epoch [148/200], Step [200/391], Loss: 0.0966\n",
            "Epoch [148/200], Step [300/391], Loss: 0.0879\n",
            "Epoch [149/200], Step [100/391], Loss: 0.1044\n",
            "Epoch [149/200], Step [200/391], Loss: 0.1216\n",
            "Epoch [149/200], Step [300/391], Loss: 0.0599\n",
            "Epoch [150/200], Step [100/391], Loss: 0.0623\n",
            "Epoch [150/200], Step [200/391], Loss: 0.1489\n",
            "Epoch [150/200], Step [300/391], Loss: 0.0517\n",
            "Epoch [151/200], Step [100/391], Loss: 0.0894\n",
            "Epoch [151/200], Step [200/391], Loss: 0.0519\n",
            "Epoch [151/200], Step [300/391], Loss: 0.0988\n",
            "Epoch [152/200], Step [100/391], Loss: 0.1136\n",
            "Epoch [152/200], Step [200/391], Loss: 0.0592\n",
            "Epoch [152/200], Step [300/391], Loss: 0.1035\n",
            "Epoch [153/200], Step [100/391], Loss: 0.1098\n",
            "Epoch [153/200], Step [200/391], Loss: 0.1003\n",
            "Epoch [153/200], Step [300/391], Loss: 0.1232\n",
            "Epoch [154/200], Step [100/391], Loss: 0.0606\n",
            "Epoch [154/200], Step [200/391], Loss: 0.0708\n",
            "Epoch [154/200], Step [300/391], Loss: 0.1231\n",
            "Epoch [155/200], Step [100/391], Loss: 0.0396\n",
            "Epoch [155/200], Step [200/391], Loss: 0.0657\n",
            "Epoch [155/200], Step [300/391], Loss: 0.0853\n",
            "Epoch [156/200], Step [100/391], Loss: 0.0276\n",
            "Epoch [156/200], Step [200/391], Loss: 0.0543\n",
            "Epoch [156/200], Step [300/391], Loss: 0.0588\n",
            "Epoch [157/200], Step [100/391], Loss: 0.0470\n",
            "Epoch [157/200], Step [200/391], Loss: 0.0153\n",
            "Epoch [157/200], Step [300/391], Loss: 0.0401\n",
            "Epoch [158/200], Step [100/391], Loss: 0.0565\n",
            "Epoch [158/200], Step [200/391], Loss: 0.0300\n",
            "Epoch [158/200], Step [300/391], Loss: 0.0561\n",
            "Epoch [159/200], Step [100/391], Loss: 0.0436\n",
            "Epoch [159/200], Step [200/391], Loss: 0.0187\n",
            "Epoch [159/200], Step [300/391], Loss: 0.0292\n",
            "Epoch [160/200], Step [100/391], Loss: 0.0358\n",
            "Epoch [160/200], Step [200/391], Loss: 0.1264\n",
            "Epoch [160/200], Step [300/391], Loss: 0.0151\n",
            "Epoch [161/200], Step [100/391], Loss: 0.0244\n",
            "Epoch [161/200], Step [200/391], Loss: 0.0084\n",
            "Epoch [161/200], Step [300/391], Loss: 0.0327\n",
            "Epoch [162/200], Step [100/391], Loss: 0.0163\n",
            "Epoch [162/200], Step [200/391], Loss: 0.0528\n",
            "Epoch [162/200], Step [300/391], Loss: 0.0383\n",
            "Epoch [163/200], Step [100/391], Loss: 0.0168\n",
            "Epoch [163/200], Step [200/391], Loss: 0.0412\n",
            "Epoch [163/200], Step [300/391], Loss: 0.0371\n",
            "Epoch [164/200], Step [100/391], Loss: 0.0958\n",
            "Epoch [164/200], Step [200/391], Loss: 0.0836\n",
            "Epoch [164/200], Step [300/391], Loss: 0.1075\n",
            "Epoch [165/200], Step [100/391], Loss: 0.0340\n",
            "Epoch [165/200], Step [200/391], Loss: 0.0491\n",
            "Epoch [165/200], Step [300/391], Loss: 0.0418\n",
            "Epoch [166/200], Step [100/391], Loss: 0.0062\n",
            "Epoch [166/200], Step [200/391], Loss: 0.0749\n",
            "Epoch [166/200], Step [300/391], Loss: 0.0449\n",
            "Epoch [167/200], Step [100/391], Loss: 0.0200\n",
            "Epoch [167/200], Step [200/391], Loss: 0.0286\n",
            "Epoch [167/200], Step [300/391], Loss: 0.0979\n",
            "Epoch [168/200], Step [100/391], Loss: 0.0047\n",
            "Epoch [168/200], Step [200/391], Loss: 0.0596\n",
            "Epoch [168/200], Step [300/391], Loss: 0.0146\n",
            "Epoch [169/200], Step [100/391], Loss: 0.0106\n",
            "Epoch [169/200], Step [200/391], Loss: 0.0442\n",
            "Epoch [169/200], Step [300/391], Loss: 0.0684\n",
            "Epoch [170/200], Step [100/391], Loss: 0.0439\n",
            "Epoch [170/200], Step [200/391], Loss: 0.0498\n",
            "Epoch [170/200], Step [300/391], Loss: 0.0202\n",
            "Epoch [171/200], Step [100/391], Loss: 0.0074\n",
            "Epoch [171/200], Step [200/391], Loss: 0.0180\n",
            "Epoch [171/200], Step [300/391], Loss: 0.0229\n",
            "Epoch [172/200], Step [100/391], Loss: 0.0138\n",
            "Epoch [172/200], Step [200/391], Loss: 0.0125\n",
            "Epoch [172/200], Step [300/391], Loss: 0.0179\n",
            "Epoch [173/200], Step [100/391], Loss: 0.0104\n",
            "Epoch [173/200], Step [200/391], Loss: 0.0226\n",
            "Epoch [173/200], Step [300/391], Loss: 0.0437\n",
            "Epoch [174/200], Step [100/391], Loss: 0.0155\n",
            "Epoch [174/200], Step [200/391], Loss: 0.0121\n",
            "Epoch [174/200], Step [300/391], Loss: 0.0173\n",
            "Epoch [175/200], Step [100/391], Loss: 0.0252\n",
            "Epoch [175/200], Step [200/391], Loss: 0.0099\n",
            "Epoch [175/200], Step [300/391], Loss: 0.0772\n",
            "Epoch [176/200], Step [100/391], Loss: 0.0240\n",
            "Epoch [176/200], Step [200/391], Loss: 0.0692\n",
            "Epoch [176/200], Step [300/391], Loss: 0.0289\n",
            "Epoch [177/200], Step [100/391], Loss: 0.0161\n",
            "Epoch [177/200], Step [200/391], Loss: 0.0127\n",
            "Epoch [177/200], Step [300/391], Loss: 0.0460\n",
            "Epoch [178/200], Step [100/391], Loss: 0.0363\n",
            "Epoch [178/200], Step [200/391], Loss: 0.0395\n",
            "Epoch [178/200], Step [300/391], Loss: 0.0137\n",
            "Epoch [179/200], Step [100/391], Loss: 0.0360\n",
            "Epoch [179/200], Step [200/391], Loss: 0.0138\n",
            "Epoch [179/200], Step [300/391], Loss: 0.0208\n",
            "Epoch [180/200], Step [100/391], Loss: 0.0063\n",
            "Epoch [180/200], Step [200/391], Loss: 0.0225\n",
            "Epoch [180/200], Step [300/391], Loss: 0.0643\n",
            "Epoch [181/200], Step [100/391], Loss: 0.0019\n",
            "Epoch [181/200], Step [200/391], Loss: 0.0114\n",
            "Epoch [181/200], Step [300/391], Loss: 0.0419\n",
            "Epoch [182/200], Step [100/391], Loss: 0.0047\n",
            "Epoch [182/200], Step [200/391], Loss: 0.0135\n",
            "Epoch [182/200], Step [300/391], Loss: 0.0595\n",
            "Epoch [183/200], Step [100/391], Loss: 0.0383\n",
            "Epoch [183/200], Step [200/391], Loss: 0.0197\n",
            "Epoch [183/200], Step [300/391], Loss: 0.0175\n",
            "Epoch [184/200], Step [100/391], Loss: 0.0380\n",
            "Epoch [184/200], Step [200/391], Loss: 0.0092\n",
            "Epoch [184/200], Step [300/391], Loss: 0.0136\n",
            "Epoch [185/200], Step [100/391], Loss: 0.0125\n",
            "Epoch [185/200], Step [200/391], Loss: 0.0163\n",
            "Epoch [185/200], Step [300/391], Loss: 0.0255\n",
            "Epoch [186/200], Step [100/391], Loss: 0.0762\n",
            "Epoch [186/200], Step [200/391], Loss: 0.0095\n",
            "Epoch [186/200], Step [300/391], Loss: 0.0283\n",
            "Epoch [187/200], Step [100/391], Loss: 0.0470\n",
            "Epoch [187/200], Step [200/391], Loss: 0.0201\n",
            "Epoch [187/200], Step [300/391], Loss: 0.0094\n",
            "Epoch [188/200], Step [100/391], Loss: 0.0101\n",
            "Epoch [188/200], Step [200/391], Loss: 0.0145\n",
            "Epoch [188/200], Step [300/391], Loss: 0.0149\n",
            "Epoch [189/200], Step [100/391], Loss: 0.0102\n",
            "Epoch [189/200], Step [200/391], Loss: 0.0137\n",
            "Epoch [189/200], Step [300/391], Loss: 0.0087\n",
            "Epoch [190/200], Step [100/391], Loss: 0.0270\n",
            "Epoch [190/200], Step [200/391], Loss: 0.0312\n",
            "Epoch [190/200], Step [300/391], Loss: 0.0226\n",
            "Epoch [191/200], Step [100/391], Loss: 0.0072\n",
            "Epoch [191/200], Step [200/391], Loss: 0.0098\n",
            "Epoch [191/200], Step [300/391], Loss: 0.0294\n",
            "Epoch [192/200], Step [100/391], Loss: 0.0177\n",
            "Epoch [192/200], Step [200/391], Loss: 0.0321\n",
            "Epoch [192/200], Step [300/391], Loss: 0.0432\n",
            "Epoch [193/200], Step [100/391], Loss: 0.0311\n",
            "Epoch [193/200], Step [200/391], Loss: 0.0403\n",
            "Epoch [193/200], Step [300/391], Loss: 0.0187\n",
            "Epoch [194/200], Step [100/391], Loss: 0.0135\n",
            "Epoch [194/200], Step [200/391], Loss: 0.0846\n",
            "Epoch [194/200], Step [300/391], Loss: 0.0172\n",
            "Epoch [195/200], Step [100/391], Loss: 0.0464\n",
            "Epoch [195/200], Step [200/391], Loss: 0.0233\n",
            "Epoch [195/200], Step [300/391], Loss: 0.0073\n",
            "Epoch [196/200], Step [100/391], Loss: 0.0385\n",
            "Epoch [196/200], Step [200/391], Loss: 0.0119\n",
            "Epoch [196/200], Step [300/391], Loss: 0.0777\n",
            "Epoch [197/200], Step [100/391], Loss: 0.0353\n",
            "Epoch [197/200], Step [200/391], Loss: 0.0273\n",
            "Epoch [197/200], Step [300/391], Loss: 0.0234\n",
            "Epoch [198/200], Step [100/391], Loss: 0.0047\n",
            "Epoch [198/200], Step [200/391], Loss: 0.0456\n",
            "Epoch [198/200], Step [300/391], Loss: 0.0104\n",
            "Epoch [199/200], Step [100/391], Loss: 0.0044\n",
            "Epoch [199/200], Step [200/391], Loss: 0.0021\n",
            "Epoch [199/200], Step [300/391], Loss: 0.0262\n",
            "Epoch [200/200], Step [100/391], Loss: 0.0461\n",
            "Epoch [200/200], Step [200/391], Loss: 0.0345\n",
            "Epoch [200/200], Step [300/391], Loss: 0.0226\n",
            "Accuracy of the network: 90.11%\n",
            "Accuracy of plane: 91.70%\n",
            "Accuracy of car: 96.10%\n",
            "Accuracy of bird: 87.00%\n",
            "Accuracy of cat: 79.80%\n",
            "Accuracy of deer: 89.80%\n",
            "Accuracy of dog: 85.20%\n",
            "Accuracy of frog: 93.80%\n",
            "Accuracy of horse: 90.50%\n",
            "Accuracy of ship: 94.40%\n",
            "Accuracy of truck: 92.80%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}